{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa02c3a",
   "metadata": {},
   "source": [
    "Databricks notebook source\n",
    "Criar uma estrutura file system\n",
    "Criação de diretórios do filestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs('/FileStore/tables/raw') # colocar os arquivos recebidosneste diretório\n",
    "dbutils.fs.mkdirs('/FileStore/tables/parquet') # depois de ler o arquivo csv, transforma o arquivo para parquet\n",
    "dbutils.fs.mkdirs('/FileStore/tables/checkpoint') # guarda os metadados dos arquivos no momento em que estão sendo lidos, para caso de falhas no nó, rede e etc, para saber onde parou o processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26ee80",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef9975",
   "metadata": {},
   "source": [
    "criação dos schemas do arquivo CSV, para o spark saber o tipo de cada um dos campos/atributos no momento da leitura\n",
    "O StructType, define a estrutura do dataframe e o StructField são os tipos/formatos dos campos/atributos, sendo TRUE/FALSE (NULL OU NOT NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import sum ,window,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167420cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"TransactionID\",IntegerType(),True),\n",
    "    StructField(\"ProductID\",IntegerType(),True),\n",
    "    StructField(\"Quantity\",IntegerType(),True),\n",
    "    StructField(\"Price\",DoubleType(),True),\n",
    "    StructField(\"Date\",TimestampType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b2130",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88787f7",
   "metadata": {},
   "source": [
    "Stream\n",
    "Leitura do arquivo para verificar processamento dos dados\n",
    "cria dataframe\n",
    "sequencia: 1 - Formato do arquivo a ser lido, nome do schema determinado, se existe ou não cabeçalho no arquivo, qual o tipo de separação do arquivo (, ou ; e etc) e de onde ele vai ler estes arquivos ( no caso o caminho onde o arquivo estará)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('csv').schema(schema).option('header',True).option('sep',';').load('/FileStore/tables/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd191c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "result_df = df.groupBy('ProductID', 'Date') \\\n",
    "    .agg(\n",
    "        sum('Quantity').alias('TotalQuantity'),\n",
    "        sum('Price').alias('TotalPrice')    \n",
    "            )\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc020f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66a0d2",
   "metadata": {},
   "source": [
    "Apaga os arquivos de maneira recursiva nas pastas/diretórios criados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c553535",
   "metadata": {},
   "source": [
    "dbutils.fs.rm('/FileStore/tables/raw',recurse =True) # colocar os arquivos recebidosneste diretório\n",
    "dbutils.fs.rm('/FileStore/tables/parquet',recurse =True) # depois de ler o arquivo csv, transforma o arquivo para parquet\n",
    "dbutils.fs.rm('/FileStore/tables/checkpoint',recurse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ffab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094af847",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8881b6",
   "metadata": {},
   "source": [
    "Ler o arquivo ,  e fazer transformação do arquivo\n",
    "Criando o Dataframe de streaming usando o schema definido e buscando o arquivo\n",
    "no diretório raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d19291",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .schema(schema)\\\n",
    "    .csv('/FileStore/tables/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf64803",
   "metadata": {},
   "source": [
    "Agregando os dados antes de salvar em parquet\n",
    "withWatermark verifica arquivos atrasados para atualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001d04d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "aggregated_df = streaming_df \\\n",
    "    .withWatermark(\"Date\",\"1 hour\") \\\n",
    "    .groupBy(\"ProductID\", window(\"Date\", \"1 hour\")) \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        sum(\"Price\").alias(\"TotalPrice\"),\n",
    "    ).select(\n",
    "        col(\"ProductID\"),\n",
    "        col(\"window\").start.alias(\"WindowsStart\"),\n",
    "        col(\"window\").end.alias(\"WindowsEnd\"),\n",
    "        col(\"TotalQuantity\"),\n",
    "        col(\"TotalPrice\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de65b7",
   "metadata": {},
   "source": [
    "Caminhos de saída e checkpoint\n",
    "criando dois df para receber os arquivos dos diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/FileStore/tables/parquet'\n",
    "checkpoint_path = '/FileStore/tables/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a escrita do stream\n",
    "# sequencia: 1 - o tipo de atualização do arquivo ( append), 2- o formato gerado do arquivo, 3 - o nome do novo diretório e caminho do arquivo,\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\",output_path)\\\n",
    "    .option(\"checkpointlocation\",checkpoint_path)\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed093fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4ad0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bcadaf5",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2f939",
   "metadata": {},
   "source": [
    "Listando arquivos no diretório especificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46de092",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709288a6",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932425f3",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca182efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/checkpoint\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
