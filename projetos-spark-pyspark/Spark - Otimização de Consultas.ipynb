{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c71951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "df = spark.read.csv(\"/FileStore/tables/sales-1.csv\", header =True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d4aba",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac197a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15806262",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf8609",
   "metadata": {},
   "source": [
    "repartition para deixar os dados iguais nos cluster\n",
    "evitar os skills \n",
    "neste caso ,ele vai criar 4 arquivos separados por 'country'\n",
    "faaz o shuffle dos dados ( redistribuição em todos os nós dos cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(4,'country')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7119477",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0185d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colocar um dataframe em cache\n",
    "# geralmente é utilizado, pra quando um dataframe é referenciado muitas vezes dentro do código, precisa passar por transformações múltiplas\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b91bb",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9de29b",
   "metadata": {},
   "source": [
    "Para um arquivo muito pequeno, você pode utilizar o broadcast.\n",
    "o broadcast, pulveriza o arquivo em todos os nós dos clusters\n",
    "funciona bem , para casos de leituras de arquivos grandes e pequenos de maneira conjunta\n",
    "evita junções desnecessárias que deixam o processamento mais lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = spark.read.csv(\"small_file.csv\", header=True, inferSchema=True)\n",
    "large_df = spark.read.csv(\"large_file.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535049c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "520baba0",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db4619",
   "metadata": {},
   "source": [
    "aplicando o BROADCAST\n",
    "join do arquivo large_df e small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = large_df.join(broadcast(small_df),\"join_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1e93a",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a17519",
   "metadata": {},
   "source": [
    "# utilizar coalesce\n",
    "# reduz o número de partições para evitar uma sobrecarga \n",
    "# reduzindo de 4 para 2 partições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.coalesce(2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
