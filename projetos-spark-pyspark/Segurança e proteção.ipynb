{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2eb2a28",
   "metadata": {},
   "source": [
    "Databricks notebook source\n",
    "MAGIC %md\n",
    "MAGIC  Mascarar informações ## segurança dos dados ## \n",
    "MAGIC utilizar uma função nativa do spark para mascarar os dados.\n",
    "MAGIC Utilizando função Reject\n",
    "MAGIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791fde0",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc9d7d",
   "metadata": {},
   "source": [
    "Mascarar informações ## segurança dos dados ## \n",
    "# utilizar uma função nativa do spark para mascarar os dados.\n",
    "# Utilizando função Reject\n",
    "# troca uma informação , por outra informação anonimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49737a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457f167",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc01151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe de apenas uma linha, apenas para realizar a anonimização dos dados\n",
    "df = spark.createDataFrame([(\"555-123-4567\",),(\"222-888-4574\",)], [\"phone_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d981c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311b091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "122a6f8c",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d02b91",
   "metadata": {},
   "source": [
    "criar um dataframe , utilizando o regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = df.withColumn('masked_phone',regexp_replace('phone_number',r'\\d{3}-\\d{3}','XXX-XXX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06313e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f100300e",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(masked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf028d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b272452",
   "metadata": {},
   "source": [
    "Utilizar o TOKEN\n",
    "METODO DBUTILS, para pegar as secrets da AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2471d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = dbutils.secrets.get(scope=\"your-secret-scope\",key=\"aws-acess-key-id\")\n",
    "aws_secret_access_key = dbutils.secrets.get(scope=\"your-secret-scope\",key=\"aws-secret-acess-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191e99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d98685e0",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9f3c5",
   "metadata": {},
   "source": [
    "Como conectar na AWS - acesso\n",
    "objeto spark session, as funcionalidades do spark no databricks são criadas automaticamente ( linguagem baixo nível)\n",
    "_jsc é a referenca a java spark context. o spark é escrito em scala e utiliza a JVM ( java virtual machine)\n",
    "o pyspark é apenas uma interface para poder utilizar a linguagem python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de73f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.acess.key\",aws_access_key_id)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236658d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ler o arquivo de dentro do s3 aws\n",
    "# o s3\"a\",pq o s3 requer autenticação pra acessar os dados e o spark vai precisar das informações pra acessar o s3, implementação feita pela\n",
    "#aws\n",
    "df = spark.read.csv(\"s3a://your-bucket-name/your-file.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
